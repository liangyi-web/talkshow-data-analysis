{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Secrets of Laughter: When, What, and Why Audiences Laugh in Talk show \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c4e0f76",
      "metadata": {},
      "source": [
        "## Step 0ï¼šç¯å¢ƒå‡†å¤‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d720458a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ å½“å‰ Python ç‰ˆæœ¬: 3.13.5\n",
            "âš ï¸ æ£€æµ‹åˆ° Python 3.13ï¼Œä¸å®Œå…¨å…¼å®¹ Whisper / SentenceTransformersã€‚å»ºè®®åˆ‡æ¢åˆ° Python 3.10 æˆ– 3.11 ç¯å¢ƒè¿è¡Œã€‚\n",
            "ğŸ¬ FFmpeg å·²æ£€æµ‹åˆ°ã€‚\n",
            "ğŸ§  ç³»ç»Ÿ: Windows 11 (AMD64)\n",
            "ğŸ“‚ å·¥ä½œç›®å½•: c:\\Users\\koto\\Desktop\\SemesterA\\5507ä½œä¸š\n",
            "âœ… ç¯å¢ƒå˜é‡é˜²æŠ¤åŠ è½½å®Œæˆï¼å¯å®‰å…¨æ‰§è¡Œ Step 0 ~ Step 4\n"
          ]
        }
      ],
      "source": [
        "# è¿™æ˜¯ä¸€ä¸ªç¯å¢ƒå‡†å¤‡çš„ä»£ç ï¼Œç¡®ä¿ä»¥ä¸‹å†…å®¹èƒ½å¤Ÿå®Œæ•´è¿è¡Œ\n",
        "\n",
        "import os, sys, platform, subprocess\n",
        "\n",
        "# === 1ï¸âƒ£ é˜²æ­¢ OMP / MKL å†²çª ===\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"   \n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"         \n",
        "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "# === 2ï¸âƒ£ ç¦ç”¨ matplotlib GUI æ¨¡å¼ï¼Œé˜²æ­¢ VSCode æ¸²æŸ“å´©æºƒ ===\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# === 3ï¸âƒ£ è‡ªåŠ¨è®¾ç½®å›½å†… pip æºï¼ˆé˜² ProxyErrorï¼‰ ===\n",
        "try:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"config\", \"set\", \"global.index-url\",\n",
        "         \"https://pypi.tuna.tsinghua.edu.cn/simple\"],\n",
        "        check=False,\n",
        "        stdout=subprocess.DEVNULL,\n",
        "        stderr=subprocess.DEVNULL\n",
        "    )\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# === 4ï¸âƒ£ æ£€æŸ¥ Python ç‰ˆæœ¬ ===\n",
        "ver = sys.version_info\n",
        "print(f\"ğŸ å½“å‰ Python ç‰ˆæœ¬: {ver.major}.{ver.minor}.{ver.micro}\")\n",
        "if ver.major == 3 and ver.minor >= 13:\n",
        "    print(\"âš ï¸ æ£€æµ‹åˆ° Python 3.13ï¼Œä¸å®Œå…¨å…¼å®¹ Whisper / SentenceTransformersã€‚å»ºè®®åˆ‡æ¢åˆ° Python 3.10 æˆ– 3.11 ç¯å¢ƒè¿è¡Œã€‚\")\n",
        "\n",
        "# === 5ï¸âƒ£ æ£€æŸ¥ FFmpeg å¯ç”¨æ€§ ===\n",
        "import shutil\n",
        "if shutil.which(\"ffmpeg\"):\n",
        "    print(\"ğŸ¬ FFmpeg å·²æ£€æµ‹åˆ°ã€‚\")\n",
        "else:\n",
        "    print(\"âš ï¸ æœªæ£€æµ‹åˆ° FFmpegï¼Œè¯·å®‰è£…å¹¶åŠ å…¥ PATHï¼ˆæˆ–æ”¾åˆ° C:\\\\ffmpeg\\\\binï¼‰ã€‚\")\n",
        "\n",
        "# === 6ï¸âƒ£ æ‰“å°ç³»ç»Ÿç¯å¢ƒä¿¡æ¯ ===\n",
        "print(f\"ğŸ§  ç³»ç»Ÿ: {platform.system()} {platform.release()} ({platform.machine()})\")\n",
        "print(f\"ğŸ“‚ å·¥ä½œç›®å½•: {os.getcwd()}\")\n",
        "print(f\"âœ… ç¯å¢ƒå˜é‡é˜²æŠ¤åŠ è½½å®Œæˆï¼å¯å®‰å…¨æ‰§è¡Œ Step 0 ~ Step 4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "97b70604",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… requests å·²å®‰è£…\n",
            "âœ… pandas å·²å®‰è£…\n",
            "âœ… matplotlib å·²å®‰è£…\n",
            "âœ… seaborn å·²å®‰è£…\n",
            "âœ… jieba å·²å®‰è£…\n",
            "âœ… networkx å·²å®‰è£…\n",
            "âœ… snownlp å·²å®‰è£…\n",
            "âœ… tqdm å·²å®‰è£…\n",
            "âœ… whisper å·²å®‰è£…\n",
            "ğŸ“¦ å®‰è£… sentence-transformers ...\n",
            "âœ… scipy å·²å®‰è£…\n",
            "\n",
            "ğŸ Python: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]\n",
            "ğŸ§  ç³»ç»Ÿ: Windows 11 AMD64\n",
            "ğŸ“‚ å·¥ä½œç›®å½•: c:\\Users\\koto\\Desktop\\SemesterA\\5507ä½œä¸š\n",
            "ğŸ¬ FFmpeg: å·²æ£€æµ‹åˆ°\n"
          ]
        }
      ],
      "source": [
        "import os, sys, subprocess, shutil, platform\n",
        "import matplotlib\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['MKL_NUM_THREADS'] = '1'\n",
        "matplotlib.use('Agg')\n",
        "try:\n",
        "    subprocess.run([sys.executable, '-m', 'pip', 'config', 'set', 'global.index-url', 'https://pypi.tuna.tsinghua.edu.cn/simple'], check=False)\n",
        "except Exception:\n",
        "    pass\n",
        "def ensure(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f'âœ… {package} å·²å®‰è£…')\n",
        "    except ImportError:\n",
        "        print(f'ğŸ“¦ å®‰è£… {package} ...')\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', package])\n",
        "pkgs = ['requests','pandas','matplotlib','seaborn','jieba','networkx','snownlp','tqdm','whisper','sentence-transformers','scipy']\n",
        "for p in pkgs:\n",
        "    ensure(p)\n",
        "print('\\nğŸ Python:', sys.version)\n",
        "print('ğŸ§  ç³»ç»Ÿ:', platform.system(), platform.release(), platform.machine())\n",
        "print('ğŸ“‚ å·¥ä½œç›®å½•:', os.getcwd())\n",
        "print('ğŸ¬ FFmpeg:', 'å·²æ£€æµ‹åˆ°' if shutil.which('ffmpeg') else 'æœªæ£€æµ‹åˆ°ï¼ˆè¯·å®‰è£…å¹¶åŠ å…¥ PATHï¼‰')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1ï¼šè…¾è®¯è§†é¢‘å¼¹å¹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00f42693",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸ æ— æ³•è·å–è§†é¢‘æ—¶é•¿: could not convert string to float: ''\n",
            "âš ï¸ ä½¿ç”¨é»˜è®¤æ—¶é•¿ 10 åˆ†é’Ÿã€‚\n",
            "ğŸ” æ­£åœ¨æµ‹è¯•å¯ç”¨æ¥å£ ...\n",
            "âœ… æ‰¾åˆ°å¯ç”¨æ¥å£: https://dm.video.qq.com/barrage/segment/{vid}/t/v1/{start}/{end}\n",
            "\n",
            "ğŸš€ å¼€å§‹æŠ“å–å¼¹å¹• (step=30000 ms) ...\n",
            "âœ… æŠ“å–ç¬¬ 0 æ®µï¼Œå…± 507 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 1 æ®µï¼Œå…± 545 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 2 æ®µï¼Œå…± 529 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 3 æ®µï¼Œå…± 326 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 4 æ®µï¼Œå…± 357 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 5 æ®µï¼Œå…± 463 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 6 æ®µï¼Œå…± 570 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 7 æ®µï¼Œå…± 377 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 8 æ®µï¼Œå…± 404 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 9 æ®µï¼Œå…± 419 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 10 æ®µï¼Œå…± 353 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 11 æ®µï¼Œå…± 305 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 12 æ®µï¼Œå…± 404 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 13 æ®µï¼Œå…± 366 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 14 æ®µï¼Œå…± 518 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 15 æ®µï¼Œå…± 429 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 16 æ®µï¼Œå…± 185 æ¡\n",
            "âœ… æŠ“å–ç¬¬ 17 æ®µï¼Œå…± 208 æ¡\n"
          ]
        }
      ],
      "source": [
        "# è¿™æ˜¯çˆ¬å–è…¾è®¯è§†é¢‘å¼¹å¹•çš„ä»£ç \n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# ================== é…ç½®åŒºåŸŸ ==================\n",
        "cover_id = \"mzc00200li7mfpr\"  # èŠ‚ç›®å°é¢ID\n",
        "vid = \"v41013vxrfa\"           # è§†é¢‘VID\n",
        "video_path = r\"C:\\Users\\koto\\Desktop\\talkshow01x.mkv\"  # æœ¬åœ°è§†é¢‘è·¯å¾„ï¼Œå¯è‡ªåŠ¨è¯»å–æ—¶é•¿\n",
        "headers = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "        \"Chrome/140.0.0.0 Safari/537.36\"\n",
        "    )\n",
        "}\n",
        "step = 30000  # æ¯æ®µæ—¶é•¿ï¼ˆæ¯«ç§’ï¼Œ30ç§’ï¼‰\n",
        "max_retry = 3\n",
        "# ============================================\n",
        "\n",
        "\n",
        "# ğŸ¬ è‡ªåŠ¨æ£€æµ‹è§†é¢‘æ€»æ—¶é•¿\n",
        "def get_video_duration(video_path: str) -> int:\n",
        "    try:\n",
        "        cmd = [\n",
        "            \"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
        "            \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\",\n",
        "            video_path\n",
        "        ]\n",
        "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        duration = float(result.stdout.strip())\n",
        "        print(f\"ğŸ¬ è‡ªåŠ¨æ£€æµ‹åˆ°è§†é¢‘æ—¶é•¿ï¼š{duration:.2f} ç§’\")\n",
        "        return int(duration * 1000)  # è½¬ä¸ºæ¯«ç§’\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ æ— æ³•è·å–è§†é¢‘æ—¶é•¿: {e}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "# ğŸŒ è‡ªåŠ¨æ£€æµ‹æ¥å£æ¨¡æ¿\n",
        "def detect_url_template(cover_id, vid):\n",
        "    url_patterns = [\n",
        "        \"https://dm.video.qq.com/barrage/segment/{vid}/t/v1/{start}/{end}\",     # æ™®é€šæ¥å£\n",
        "        \"https://dm.video.qq.com/barrage/segment/{cover}/{vid}/t/v1/{start}/{end}\",  # å«coveræ¥å£\n",
        "        \"https://dm.video.qq.com/barrage/base/{vid}?barrage_segment={start}\",   # æ—§ç‰ˆæ¥å£\n",
        "    ]\n",
        "    print(\"ğŸ” æ­£åœ¨æµ‹è¯•å¯ç”¨æ¥å£ ...\")\n",
        "\n",
        "    for pattern in url_patterns:\n",
        "        test_url = pattern.format(cover=cover_id, vid=vid, start=0, end=30000)\n",
        "        try:\n",
        "            r = requests.get(test_url, headers=headers, timeout=8)\n",
        "            if r.status_code == 200 and \"barrage\" in r.text:\n",
        "                print(f\"âœ… æ‰¾åˆ°å¯ç”¨æ¥å£: {pattern}\")\n",
        "                return pattern\n",
        "            else:\n",
        "                print(f\"âŒ æ— æ•ˆæ¥å£: {pattern} (status {r.status_code})\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ æµ‹è¯•å¤±è´¥: {pattern} â†’ {e}\")\n",
        "    print(\"ğŸ›‘ æœªæ‰¾åˆ°å¯ç”¨çš„å¼¹å¹•æ¥å£ã€‚\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# ğŸ’¬ æŠ“å–å•ä¸ªæ—¶é—´æ®µå¼¹å¹•\n",
        "def fetch_segment(url, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            return resp.json()\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ è¯·æ±‚å¤±è´¥ï¼ˆç¬¬ {attempt + 1} æ¬¡ï¼‰: {e}\")\n",
        "            time.sleep(1)\n",
        "    return None\n",
        "\n",
        "\n",
        "# æˆ‘è¦å¼€å§‹æŠ“å¼¹å¹•å•¦ï¼\n",
        "def crawl_danmu(cover_id, vid, video_path):\n",
        "    # 1. è·å–æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰\n",
        "    duration_ms = get_video_duration(video_path)\n",
        "    if duration_ms == 0:\n",
        "        print(\"âš ï¸ ä½¿ç”¨é»˜è®¤æ—¶é•¿ 10 åˆ†é’Ÿã€‚\")\n",
        "        duration_ms = 600_000\n",
        "\n",
        "    # 2. æ£€æµ‹æ¥å£æ¨¡æ¿\n",
        "    template = detect_url_template(cover_id, vid)\n",
        "    if not template:\n",
        "        return\n",
        "\n",
        "    # 3. æŠ“å–å¼¹å¹•\n",
        "    print(f\"\\nğŸš€ å¼€å§‹æŠ“å–å¼¹å¹• (step={step} ms) ...\")\n",
        "    all_danmu = []\n",
        "    rounds = duration_ms // step + 1\n",
        "\n",
        "    for i in range(rounds):\n",
        "        start = i * step\n",
        "        end = start + step\n",
        "        url = template.format(cover=cover_id, vid=vid, start=start, end=end)\n",
        "\n",
        "        data = fetch_segment(url)\n",
        "        if not data:\n",
        "            print(f\"ğŸ›‘ ç¬¬ {i} æ®µæŠ“å–å¤±è´¥ï¼Œè·³è¿‡ã€‚\")\n",
        "            continue\n",
        "\n",
        "        barrage_list = data.get(\"barrage_list\") or data.get(\"barrage_list_v2\") or []\n",
        "        if not barrage_list:\n",
        "            print(f\"ğŸ›‘ ç¬¬ {i} æ®µæ²¡æœ‰å¼¹å¹•ï¼Œç»“æŸæŠ“å–ã€‚\")\n",
        "            break\n",
        "\n",
        "        print(f\"âœ… æŠ“å–ç¬¬ {i} æ®µï¼Œå…± {len(barrage_list)} æ¡\")\n",
        "        for item in barrage_list:\n",
        "            offset_ms = item.get(\"time_offset\", item.get(\"start\", 0))\n",
        "            offset_sec = round(float(offset_ms) / 1000, 2)\n",
        "            offset_mmss = f\"{int(offset_sec // 60):02d}:{int(offset_sec % 60):02d}\"\n",
        "\n",
        "            send_ts = item.get(\"create_time\", \"\")\n",
        "            if send_ts:\n",
        "                try:\n",
        "                    send_time_local = datetime.fromtimestamp(int(send_ts)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                except:\n",
        "                    send_time_local = \"\"\n",
        "            else:\n",
        "                send_time_local = \"\"\n",
        "\n",
        "            all_danmu.append({\n",
        "                \"video_id\": vid,\n",
        "                \"offset_time_sec\": offset_sec,\n",
        "                \"offset_time_mmss\": offset_mmss,\n",
        "                \"content\": item.get(\"content\", \"\"),\n",
        "                \"send_time\": send_time_local,\n",
        "                \"user_name\": (\n",
        "                    item.get(\"user\", {}).get(\"nick\", \"\")\n",
        "                    if isinstance(item.get(\"user\"), dict)\n",
        "                    else item.get(\"nick\", \"\")\n",
        "                ),\n",
        "                \"color\": item.get(\"color\", \"\"),\n",
        "                \"mode\": item.get(\"mode\", \"\"),\n",
        "            })\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    # 4. ä¿å­˜ç»“æœï¼ˆCSV + JSONï¼‰\n",
        "    if all_danmu:\n",
        "        df = pd.DataFrame(all_danmu)\n",
        "        df.sort_values(by=\"offset_time_sec\", inplace=True)\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        desktop = os.path.expanduser(\"~/Desktop\")\n",
        "        csv_path = os.path.join(desktop, f\"{vid}_full_danmu_sorted.csv\")\n",
        "        json_path = os.path.join(desktop, f\"{vid}_danmu_raw.json\")\n",
        "\n",
        "        # ä¿å­˜ CSV æ–‡ä»¶\n",
        "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "        # ä¿å­˜ JSON æ–‡ä»¶\n",
        "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(all_danmu, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"\\nğŸ‰ å…±æŠ“å– {len(df)} æ¡å¼¹å¹•ï¼š\")\n",
        "        print(f\"ğŸ“„ CSV æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{csv_path}\")\n",
        "        print(f\"ğŸ“˜ JSON æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{json_path}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•å¼¹å¹•ã€‚\")\n",
        "\n",
        "\n",
        "# ================== å¯åŠ¨æ‰§è¡Œ ==================\n",
        "if __name__ == \"__main__\":\n",
        "    crawl_danmu(cover_id, vid, video_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "524589db",
      "metadata": {},
      "source": [
        "## Step 2ï¼šè…¾è®¯è§†é¢‘å­—å¹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a2184467",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tesseract å¯ç”¨è¯­è¨€ï¼š ['chi_sim', 'eng', 'osd']\n",
            "â–¶ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨å¹¶è‡ªåŠ¨æ’­æ”¾è§†é¢‘ã€æ‰“å¼€å­—å¹•...\n",
            "Playwright å­è„šæœ¬è·¯å¾„: C:\\Users\\koto\\AppData\\Local\\Temp\\tencent_playwright_driver_tmp.py\n",
            "å±å¹•å°ºå¯¸: 2880 x 1800\n",
            "å¼€å§‹å½•åˆ¶å­—å¹•...\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 494\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28mprint\u001b[39m(r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâ†’\u001b[39m\u001b[38;5;124m\"\u001b[39m, r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend_sec\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# çœŸæ­£æ‰§è¡Œ\u001b[39;00m\n\u001b[1;32m--> 494\u001b[0m capture_subtitles_from_screen()\n",
            "Cell \u001b[1;32mIn[1], line 380\u001b[0m, in \u001b[0;36mcapture_subtitles_from_screen\u001b[1;34m()\u001b[0m\n\u001b[0;32m    378\u001b[0m t_sec \u001b[38;5;241m=\u001b[39m now \u001b[38;5;241m-\u001b[39m start_wall\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t_sec \u001b[38;5;241m<\u001b[39m SKIP_FIRST_SECONDS:\n\u001b[1;32m--> 380\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(interval)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    383\u001b[0m shot \u001b[38;5;241m=\u001b[39m sct\u001b[38;5;241m.\u001b[39mgrab(monitor)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# é€šè¿‡OCRæ–¹æ³•æ‹¿åˆ°è…¾è®¯è§†é¢‘å­—å¹•\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import subprocess\n",
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "import mss\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "\n",
        "# ================== ä½ çš„è…¾è®¯è§†é¢‘é“¾æ¥ ==================\n",
        "TENCENT_URL = \"https://v.qq.com/x/cover/mzc00200li7mfpr/v41013vxrfa.html\"\n",
        "\n",
        "# ================== å…¨å±€é…ç½® ==================\n",
        "\n",
        "MAX_SECONDS = 4 * 60        # â˜… æ€»é‡‡é›†æ—¶é•¿ï¼š4 åˆ†é’Ÿ\n",
        "CAPTURE_FPS = 2             # æ¯ç§’é‡‡æ ·æ¬¡æ•°\n",
        "\n",
        "SKIP_FIRST_SECONDS = 25     # å‰ 25 ç§’ä¸€å¾‹ä¸¢æ‰ï¼ˆå¼€å¤´å¹¿å‘Šç­‰ï¼‰\n",
        "\n",
        "PLAYWRIGHT_ALIVE_EXTRA = 10  # æµè§ˆå™¨é¢å¤–å­˜æ´»æ—¶é—´ï¼ˆç§’ï¼‰\n",
        "\n",
        "# â˜† å­—å¹•æ‰€åœ¨å¤§è‡´åŒºåŸŸï¼ˆç›¸å¯¹å±å¹•é«˜åº¦/å®½åº¦ï¼‰\n",
        "#   - çºµå‘ï¼šåº•éƒ¨ä¸€æ¡å¸¦ï¼Œç´§è´´è¿›åº¦æ¡ä¸Šæ–¹\n",
        "#   - æ¨ªå‘ï¼šåªå–ä¸­é—´ 40% å®½åº¦ï¼Œé¿å¼€å·¦ä¾§ä¼šå‘˜å¹¿å‘Š & å³ä¸‹è§’æ—¶é—´\n",
        "SUBTITLE_TOP_RATIO    = 0.75\n",
        "SUBTITLE_BOTTOM_RATIO = 0.90\n",
        "SUBTITLE_LEFT_RATIO   = 0.30\n",
        "SUBTITLE_RIGHT_RATIO  = 0.70\n",
        "\n",
        "# OCR ç›¸å…³ï¼ˆchi_sim è¦è£…åœ¨é»˜è®¤ tessdata ç›®å½•ï¼‰\n",
        "TESSERACT_EXE = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
        "OCR_LANG      = \"chi_sim\"\n",
        "BIN_THRESHOLD = 180          # äºŒå€¼åŒ–é˜ˆå€¼ï¼Œå¯ 160~200 å¾®è°ƒ\n",
        "MIN_LINE_CHARS = 2\n",
        "MERGE_MAX_GAP  = 0.8         # åŒä¸€æ¡å­—å¹•å…è®¸çš„æœ€å¤§æ—¶é—´é—´éš™ï¼ˆç§’ï¼‰\n",
        "\n",
        "# è¾“å‡ºä½ç½®\n",
        "DESKTOP = Path.home() / \"Desktop\"\n",
        "OUT_CSV = DESKTOP / \"captions.csv\"\n",
        "OUT_SRT = DESKTOP / \"captions.srt\"\n",
        "\n",
        "# è°ƒè¯•æˆªå›¾\n",
        "DEBUG_SAVE_IMAGES = True\n",
        "DEBUG_SAVE_LIMIT  = 8\n",
        "DEBUG_DIR         = DESKTOP / \"caption_debug_screen\"\n",
        "\n",
        "\n",
        "# ================== å·¥å…·å‡½æ•° ==================\n",
        "\n",
        "def sec_to_srt(t: float) -> str:\n",
        "    if t < 0:\n",
        "        t = 0.0\n",
        "    h = int(t // 3600)\n",
        "    m = int((t % 3600) // 60)\n",
        "    s = int(t % 60)\n",
        "    ms = int(round((t - int(t)) * 1000))\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n",
        "\n",
        "\n",
        "def edit_distance(a: str, b: str) -> int:\n",
        "    if a == b:\n",
        "        return 0\n",
        "    la, lb = len(a), len(b)\n",
        "    if la == 0:\n",
        "        return lb\n",
        "    if lb == 0:\n",
        "        return la\n",
        "    dp = [[0] * (lb + 1) for _ in range(la + 1)]\n",
        "    for i in range(la + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(lb + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, la + 1):\n",
        "        for j in range(1, lb + 1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(\n",
        "                dp[i-1][j] + 1,\n",
        "                dp[i][j-1] + 1,\n",
        "                dp[i-1][j-1] + cost,\n",
        "            )\n",
        "    return dp[la][lb]\n",
        "\n",
        "\n",
        "def similar_text(a: str, b: str, max_ratio: float = 0.2) -> bool:\n",
        "    if not a or not b:\n",
        "        return False\n",
        "    dist = edit_distance(a, b)\n",
        "    norm = dist / max(len(a), len(b))\n",
        "    return norm <= max_ratio\n",
        "\n",
        "\n",
        "def merge_segments(segments, max_gap=0.6):\n",
        "    if not segments:\n",
        "        return []\n",
        "    segments.sort(key=lambda x: x[0])\n",
        "    out = [list(segments[0])]\n",
        "    for s, e, tx in segments[1:]:\n",
        "        ps, pe, ptx = out[-1]\n",
        "        if (s - pe) <= max_gap and similar_text(tx, ptx):\n",
        "            out[-1][1] = max(pe, e)\n",
        "            if len(tx) > len(ptx):\n",
        "                out[-1][2] = tx\n",
        "        else:\n",
        "            out.append([s, e, tx])\n",
        "    return [tuple(x) for x in out]\n",
        "\n",
        "\n",
        "def ensure_ocr_ready():\n",
        "    pytesseract.pytesseract.tesseract_cmd = TESSERACT_EXE\n",
        "    os.environ.pop(\"TESSDATA_PREFIX\", None)\n",
        "    try:\n",
        "        langs = pytesseract.get_languages(config=\"\")\n",
        "        print(\"Tesseract å¯ç”¨è¯­è¨€ï¼š\", langs)\n",
        "        if OCR_LANG not in langs:\n",
        "            print(\"âš ï¸ chi_sim ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè¯´æ˜ä¸­æ–‡è¯­è¨€åŒ…è¿˜æ²¡å®‰è£…å¥½ã€‚\")\n",
        "    except Exception as e:\n",
        "        print(\"âš ï¸ è¯­è¨€åˆ—è¡¨æ£€æµ‹å¤±è´¥ï¼š\", e)\n",
        "\n",
        "\n",
        "def binarize(img, threshold=BIN_THRESHOLD):\n",
        "    \"\"\"å¢å¼º + æ”¾å¤§ + äºŒå€¼åŒ–ï¼Œæé«˜å°å­—å¹•è¯†åˆ«ç‡ã€‚\"\"\"\n",
        "    g = img.convert(\"L\")\n",
        "    g = ImageOps.autocontrast(g)\n",
        "    g = g.filter(ImageFilter.MedianFilter(3))\n",
        "    g = g.resize((g.width * 2, g.height * 2), Image.BICUBIC)\n",
        "    bw = g.point(lambda p: 255 if p > threshold else 0, mode=\"1\")\n",
        "    return bw\n",
        "\n",
        "\n",
        "def has_chinese(s: str) -> bool:\n",
        "    return any(\"\\u4e00\" <= ch <= \"\\u9fff\" for ch in s)\n",
        "\n",
        "\n",
        "def chinese_char_count(s: str) -> int:\n",
        "    return sum(1 for ch in s if \"\\u4e00\" <= ch <= \"\\u9fff\")\n",
        "\n",
        "\n",
        "def extract_center_subtitle(crop_img: Image.Image) -> str | None:\n",
        "    \"\"\"\n",
        "    åœ¨æˆªä¸‹æ¥çš„æ¨ªæ¡é‡Œï¼Œç”¨ image_to_data åªå–\n",
        "    â€œæœ€æ¥è¿‘æ¡å¸¦å‚ç›´ä¸­å¿ƒçš„é‚£ä¸€è¡Œä¸­æ–‡â€ ä½œä¸ºå­—å¹•ã€‚\n",
        "    \"\"\"\n",
        "    bw = binarize(crop_img, BIN_THRESHOLD)\n",
        "    cfg = \"--psm 6 --oem 3\"\n",
        "    data = pytesseract.image_to_data(\n",
        "        bw, lang=OCR_LANG, config=cfg, output_type=Output.DICT\n",
        "    )\n",
        "\n",
        "    n = len(data[\"text\"])\n",
        "    h = bw.height\n",
        "    center_y = h / 2.0\n",
        "\n",
        "    # æŒ‰ line_num èšåˆ\n",
        "    line_dict = {}\n",
        "    for i in range(n):\n",
        "        txt = data[\"text\"][i].strip()\n",
        "        if not txt:\n",
        "            continue\n",
        "        try:\n",
        "            conf = int(data[\"conf\"][i])\n",
        "        except ValueError:\n",
        "            conf = 0\n",
        "        if conf < 60:\n",
        "            continue\n",
        "        if not has_chinese(txt):\n",
        "            continue\n",
        "\n",
        "        line_no = data[\"line_num\"][i]\n",
        "        left = data[\"left\"][i]\n",
        "        top = data[\"top\"][i]\n",
        "        height = data[\"height\"][i]\n",
        "        y_mid = top + height / 2.0\n",
        "\n",
        "        line_dict.setdefault(line_no, []).append(\n",
        "            dict(text=txt, x=left, y=y_mid)\n",
        "        )\n",
        "\n",
        "    if not line_dict:\n",
        "        return None\n",
        "\n",
        "    # é€‰ï¼šç¦»æ¡å¸¦ä¸­å¿ƒæœ€è¿‘ã€ä¸”æ±‰å­—æ•°é‡æœ€å¤šçš„é‚£ä¸€è¡Œ\n",
        "    best_line = None\n",
        "    best_score = None\n",
        "    for line_no, items in line_dict.items():\n",
        "        ys = [it[\"y\"] for it in items]\n",
        "        avg_y = sum(ys) / len(ys)\n",
        "        line_txt = \"\".join([it[\"text\"] for it in sorted(items, key=lambda x: x[\"x\"])])\n",
        "        ch_cnt = chinese_char_count(line_txt)\n",
        "        score = abs(avg_y - center_y) - ch_cnt * 2.0\n",
        "        if best_score is None or score < best_score:\n",
        "            best_score = score\n",
        "            best_line = line_txt\n",
        "\n",
        "    if not best_line:\n",
        "        return None\n",
        "\n",
        "    cleaned = re.sub(r\"[^\\u4e00-\\u9fffï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€â€œâ€â€˜â€™â€¦Â·0-9A-Za-z]\", \"\", best_line)\n",
        "    if len(cleaned) < MIN_LINE_CHARS:\n",
        "        return None\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "# ================== å­è¿›ç¨‹ï¼šPlaywright æ§åˆ¶æµè§ˆå™¨ ==================\n",
        "\n",
        "def launch_browser_with_playwright():\n",
        "    temp_dir = Path(tempfile.gettempdir())\n",
        "    script_path = temp_dir / \"tencent_playwright_driver_tmp.py\"\n",
        "\n",
        "    playwright_script = f\"\"\"\\\n",
        "from playwright.sync_api import sync_playwright\n",
        "import time\n",
        "\n",
        "URL = {TENCENT_URL!r}\n",
        "PLAY_SECONDS = {MAX_SECONDS + PLAYWRIGHT_ALIVE_EXTRA}\n",
        "\n",
        "def main():\n",
        "    with sync_playwright() as p:\n",
        "        try:\n",
        "            browser = p.chromium.launch(\n",
        "                headless=False,\n",
        "                channel=\"chrome\",\n",
        "                args=[\"--disable-blink-features=AutomationControlled\"],\n",
        "            )\n",
        "        except Exception:\n",
        "            browser = p.chromium.launch(\n",
        "                headless=False,\n",
        "                args=[\"--disable-blink-features=AutomationControlled\"],\n",
        "            )\n",
        "\n",
        "        ctx = browser.new_context(\n",
        "            viewport={{\"width\": 1366, \"height\": 820}},\n",
        "            user_agent=(\n",
        "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                \"Chrome/120.0 Safari/537.36\"\n",
        "            ),\n",
        "        )\n",
        "        page = ctx.new_page()\n",
        "        try:\n",
        "            page.goto(URL, wait_until=\"domcontentloaded\", timeout=90_000)\n",
        "        except Exception:\n",
        "            time.sleep(5)\n",
        "\n",
        "        # æ’­æ”¾\n",
        "        play_selectors = [\n",
        "            'button[aria-label*=\"æ’­æ”¾\"]',\n",
        "            'div[class*=\"txp_button_play\"]',\n",
        "            'div[class*=\"txp_btn\"]',\n",
        "            'button[class*=\"txp\"]',\n",
        "            'div.txp_btn_play',\n",
        "        ]\n",
        "        for sel in play_selectors:\n",
        "            try:\n",
        "                btn = page.query_selector(sel)\n",
        "                if btn:\n",
        "                    btn.click()\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "        # å…¨å±\n",
        "        fullscreen_selectors = [\n",
        "            'button[aria-label*=\"å…¨å±\"]',\n",
        "            'div[class*=\"txp_button_fullscreen\"]',\n",
        "            'div.txp_btn_fullscreen',\n",
        "        ]\n",
        "        for sel in fullscreen_selectors:\n",
        "            try:\n",
        "                b = page.query_selector(sel)\n",
        "                if b:\n",
        "                    b.click()\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        # æ‰“å¼€â€œè¯­è¨€â€\n",
        "        lang_selectors = [\n",
        "            'text=è¯­è¨€',\n",
        "            'button[aria-label*=\"è¯­è¨€\"]',\n",
        "            'div[aria-label*=\"è¯­è¨€\"]',\n",
        "            'span:has-text(\"è¯­è¨€\")',\n",
        "        ]\n",
        "        for sel in lang_selectors:\n",
        "            try:\n",
        "                el = page.query_selector(sel)\n",
        "                if el:\n",
        "                    el.click()\n",
        "                    time.sleep(0.5)\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # é€‰æ‹©ä¸­æ–‡å­—å¹•\n",
        "        subtitle_options = [\n",
        "            'text=ç®€ä½“ä¸­æ–‡',\n",
        "            'text=ä¸­æ–‡ï¼ˆç®€ä½“ï¼‰',\n",
        "            'text=ä¸­æ–‡å­—å¹•',\n",
        "            'text=ä¸­æ–‡',\n",
        "        ]\n",
        "        found_sub = False\n",
        "        for sel in subtitle_options:\n",
        "            try:\n",
        "                item = page.query_selector(sel)\n",
        "                if item:\n",
        "                    item.click()\n",
        "                    found_sub = True\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        if found_sub:\n",
        "            print(\"âœ… å·²å°è¯•å¼€å¯ä¸­æ–‡å­—å¹•\")\n",
        "        else:\n",
        "            print(\"âš ï¸ æœªèƒ½è‡ªåŠ¨æ‰¾åˆ°å­—å¹•èœå•é¡¹ï¼Œå¯èƒ½ä»éœ€äººå·¥ç¡®è®¤ã€‚\")\n",
        "\n",
        "        time.sleep(PLAY_SECONDS)\n",
        "        ctx.close()\n",
        "        browser.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "    script_path.write_text(playwright_script, encoding=\"utf-8\")\n",
        "    print(\"Playwright å­è„šæœ¬è·¯å¾„:\", script_path)\n",
        "    proc = subprocess.Popen([sys.executable, str(script_path)])\n",
        "    return proc\n",
        "\n",
        "\n",
        "# ================== ä¸»æµç¨‹ï¼šå±å¹•æˆªå›¾ + OCR ==================\n",
        "\n",
        "def capture_subtitles_from_screen():\n",
        "    ensure_ocr_ready()\n",
        "\n",
        "    segments = []\n",
        "    active_text = None\n",
        "    active_start = None\n",
        "\n",
        "    DESKTOP.mkdir(parents=True, exist_ok=True)\n",
        "    if DEBUG_SAVE_IMAGES:\n",
        "        DEBUG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(\"â–¶ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨å¹¶è‡ªåŠ¨æ’­æ”¾è§†é¢‘ã€æ‰“å¼€å­—å¹•...\")\n",
        "    browser_proc = launch_browser_with_playwright()\n",
        "\n",
        "    # ç»™æµè§ˆå™¨ä¸€ç‚¹æ—¶é—´åŠ è½½&è¿›ç‰‡å¤´\n",
        "    time.sleep(10)\n",
        "\n",
        "    interval = 1.0 / max(0.1, CAPTURE_FPS)\n",
        "    start_wall = time.time()\n",
        "    debug_saved = 0\n",
        "\n",
        "    with mss.mss() as sct:\n",
        "        monitor = sct.monitors[1]\n",
        "\n",
        "        screen_width = monitor[\"width\"]\n",
        "        screen_height = monitor[\"height\"]\n",
        "        screen_left = monitor[\"left\"]\n",
        "        screen_top = monitor[\"top\"]\n",
        "\n",
        "        print(\"å±å¹•å°ºå¯¸:\", screen_width, \"x\", screen_height)\n",
        "        print(\"å¼€å§‹å½•åˆ¶å­—å¹•...\\n\")\n",
        "\n",
        "        while True:\n",
        "            now = time.time()\n",
        "            if now - start_wall > MAX_SECONDS:\n",
        "                break\n",
        "\n",
        "            t_sec = now - start_wall\n",
        "            if t_sec < SKIP_FIRST_SECONDS:\n",
        "                time.sleep(interval)\n",
        "                continue\n",
        "\n",
        "            shot = sct.grab(monitor)\n",
        "            img = Image.frombytes(\"RGB\", shot.size, shot.rgb)\n",
        "\n",
        "            # åªæˆªåº•éƒ¨ä¸­é—´ä¸€æ¡å¸¦\n",
        "            crop_left   = int(screen_left + screen_width  * SUBTITLE_LEFT_RATIO)\n",
        "            crop_right  = int(screen_left + screen_width  * SUBTITLE_RIGHT_RATIO)\n",
        "            crop_top    = int(screen_top  + screen_height * SUBTITLE_TOP_RATIO)\n",
        "            crop_bottom = int(screen_top  + screen_height * SUBTITLE_BOTTOM_RATIO)\n",
        "\n",
        "            crop = img.crop((\n",
        "                crop_left   - screen_left,\n",
        "                crop_top    - screen_top,\n",
        "                crop_right  - screen_left,\n",
        "                crop_bottom - screen_top,\n",
        "            ))\n",
        "\n",
        "            if DEBUG_SAVE_IMAGES and debug_saved < DEBUG_SAVE_LIMIT:\n",
        "                crop.save(DEBUG_DIR / f\"strip_{debug_saved:03d}.png\")\n",
        "                debug_saved += 1\n",
        "\n",
        "            try:\n",
        "                final_txt = extract_center_subtitle(crop)\n",
        "            except pytesseract.TesseractError as e:\n",
        "                print(\"âŒ Tesseract è°ƒç”¨å¤±è´¥ï¼š\", e)\n",
        "                break\n",
        "\n",
        "            if final_txt:\n",
        "                if active_text is None:\n",
        "                    active_text = final_txt\n",
        "                    active_start = t_sec\n",
        "                else:\n",
        "                    if similar_text(final_txt, active_text):\n",
        "                        # åŒä¸€å¥å­—å¹•ï¼ŒæŒç»­æ—¶é—´æ‹‰é•¿\n",
        "                        pass\n",
        "                    else:\n",
        "                        # æ–°çš„ä¸€å¥å­—å¹•ï¼Œå…ˆæ”¶å°¾ä¸Šä¸€å¥\n",
        "                        if active_start is not None:\n",
        "                            end_t = t_sec\n",
        "                            segments.append(\n",
        "                                (active_start,\n",
        "                                 max(active_start + 0.5, end_t),\n",
        "                                 active_text)\n",
        "                            )\n",
        "                        active_text = final_txt\n",
        "                        active_start = t_sec\n",
        "            else:\n",
        "                if active_text is not None and active_start is not None:\n",
        "                    end_t = t_sec\n",
        "                    segments.append(\n",
        "                        (active_start,\n",
        "                         max(active_start + 0.5, end_t),\n",
        "                         active_text)\n",
        "                    )\n",
        "                    active_text = None\n",
        "                    active_start = None\n",
        "\n",
        "            time.sleep(interval)\n",
        "\n",
        "    try:\n",
        "        browser_proc.terminate()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    if active_text is not None and active_start is not None:\n",
        "        end_t = time.time() - start_wall\n",
        "        segments.append(\n",
        "            (active_start, max(active_start + 0.5, end_t), active_text)\n",
        "        )\n",
        "\n",
        "    segments = merge_segments(segments, max_gap=MERGE_MAX_GAP)\n",
        "\n",
        "    rows = []\n",
        "    for i, (s, e, tx) in enumerate(segments, 1):\n",
        "        rows.append(\n",
        "            dict(\n",
        "                index=i,\n",
        "                start_sec=round(s, 3),\n",
        "                end_sec=round(e, 3),\n",
        "                duration=round(max(0.0, e - s), 3),\n",
        "                text=tx,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # CSVï¼šgbk æ–¹ä¾¿ Excel\n",
        "    with open(OUT_CSV, \"w\", encoding=\"gbk\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(\n",
        "            f, fieldnames=[\"index\", \"start_sec\", \"end_sec\", \"duration\", \"text\"]\n",
        "        )\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            w.writerow(r)\n",
        "\n",
        "    # SRTï¼šutf-8-sig\n",
        "    with open(OUT_SRT, \"w\", encoding=\"utf-8-sig\") as f:\n",
        "        for i, (s, e, tx) in enumerate(segments, 1):\n",
        "            f.write(str(i) + \"\\n\")\n",
        "            f.write(f\"{sec_to_srt(s)} --> {sec_to_srt(e)}\\n\")\n",
        "            lines = [t for t in re.split(r\"[\\n\\r]+\", tx) if t.strip()]\n",
        "            if len(lines) > 2:\n",
        "                lines = lines[:2]\n",
        "            f.write(\"\\n\".join(lines) + \"\\n\\n\")\n",
        "\n",
        "    print(\"\\nâœ… å½•åˆ¶å®Œæˆ\")\n",
        "    print(\"CSV:\", OUT_CSV)\n",
        "    print(\"SRT:\", OUT_SRT)\n",
        "    print(f\"å…± {len(segments)} æ¡å­—å¹•æ®µï¼ˆå‰ 5 æ¡é¢„è§ˆï¼‰ï¼š\")\n",
        "    for r in rows[:5]:\n",
        "        print(r[\"start_sec\"], \"â†’\", r[\"end_sec\"], \":\", r[\"text\"])\n",
        "\n",
        "\n",
        "# çœŸæ­£æ‰§è¡Œ\n",
        "capture_subtitles_from_screen()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3ï¼šæ•°æ®åˆæ­¥æ¸…æ´—â€”â€”whisperéªŒè¯ã€æ–‡æœ¬å¯¹é½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b09b3ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "# è¿™æ˜¯WhisperéªŒè¯\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from snownlp import SnowNLP\n",
        "\n",
        "# ========== é…ç½®åŒºåŸŸ ==========\n",
        "USE_SEMANTIC_ALIGNMENT = True  # æ˜¯å¦å¯ç”¨è¯­ä¹‰å¯¹é½ï¼ˆéœ€è¦ sentence-transformersï¼‰\n",
        "vid = \"v41013vxrfa\"            # è§†é¢‘ID\n",
        "desktop = Path(os.path.expanduser(\"~/Desktop\"))\n",
        "segments_path = desktop / \"segments.json\"               # Whisper è¾“å‡º\n",
        "danmu_path = desktop / f\"{vid}_danmu_raw.json\"          # å¼¹å¹•åŸå§‹æ•°æ®\n",
        "out_csv = desktop / f\"{vid}_danmu_aligned.csv\"          # è¾“å‡ºCSV\n",
        "out_json = desktop / f\"{vid}_danmu_aligned.json\"        # è¾“å‡ºJSON\n",
        "# =============================\n",
        "\n",
        "\n",
        "# ========== è½½å…¥æ•°æ® ==========\n",
        "if not segments_path.exists():\n",
        "    raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ° Whisper è¯­éŸ³è¯†åˆ«ç»“æœæ–‡ä»¶: {segments_path}\")\n",
        "if not danmu_path.exists():\n",
        "    raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°å¼¹å¹•æ–‡ä»¶: {danmu_path}\")\n",
        "\n",
        "segments = json.load(open(segments_path, encoding='utf-8'))\n",
        "danmu_raw = json.load(open(danmu_path, encoding='utf-8'))\n",
        "df = pd.DataFrame(danmu_raw)\n",
        "\n",
        "print(f\"ğŸ“¦ å¼¹å¹•è½½å…¥ï¼š{len(df)} æ¡\")\n",
        "print(f\"ğŸ§ è¯­éŸ³ç‰‡æ®µï¼š{len(segments)} æ®µ\")\n",
        "\n",
        "# ========== åŸºç¡€æ—¶é—´åŒ¹é… ==========\n",
        "def coarse_match_text(t, segs):\n",
        "    \"\"\"æ ¹æ®æ—¶é—´ç²—åŒ¹é… Whisper è¯­éŸ³ç‰‡æ®µ\"\"\"\n",
        "    for s in segs:\n",
        "        if s['start'] <= t <= s['end']:\n",
        "            return s['text']\n",
        "    return ''\n",
        "\n",
        "df['matched_text'] = df['offset_time_sec'].apply(lambda t: coarse_match_text(t, segments))\n",
        "df['aligned_time'] = df['offset_time_sec']\n",
        "df['aligned_text'] = df['matched_text']\n",
        "df['similarity'] = np.nan\n",
        "\n",
        "\n",
        "# ========== è¯­ä¹‰å¯¹é½ ==========\n",
        "if USE_SEMANTIC_ALIGNMENT:\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer, util\n",
        "        print(\"ğŸ§  æ­£åœ¨è¿›è¡Œè¯­ä¹‰å¯¹é½ ...ï¼ˆåŠ è½½æ¨¡å‹ä¸­ï¼‰\")\n",
        "\n",
        "        sim_model = SentenceTransformer('shibing624/text2vec-base-chinese')\n",
        "        seg_texts = [s['text'] for s in segments]\n",
        "        emb_segments = sim_model.encode(seg_texts, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "        def sem_align(text):\n",
        "            \"\"\"è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…\"\"\"\n",
        "            if not text.strip():\n",
        "                return pd.Series([np.nan, '', 0.0])\n",
        "            e = sim_model.encode(text, convert_to_tensor=True)\n",
        "            scores = util.cos_sim(e, emb_segments)[0]\n",
        "            best_idx = int(np.argmax(scores))\n",
        "            best_seg = segments[best_idx]\n",
        "            return pd.Series([best_seg['start'], best_seg['text'], float(scores[best_idx])])\n",
        "\n",
        "        df[['aligned_time', 'aligned_text', 'similarity']] = df['content'].apply(\n",
        "            lambda x: sem_align(str(x))\n",
        "        )\n",
        "\n",
        "        # å¯¹é½å¤±è´¥å›é€€åˆ°ç²—åŒ¹é…\n",
        "        mask = df['aligned_time'].isna()\n",
        "        df.loc[mask, 'aligned_time'] = df.loc[mask, 'offset_time_sec']\n",
        "        df.loc[mask, 'aligned_text'] = df.loc[mask, 'matched_text']\n",
        "\n",
        "        print(\"âœ… è¯­ä¹‰å¯¹é½å®Œæˆ\")\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"âš ï¸ è¯­ä¹‰å¯¹é½è·³è¿‡ï¼š{e}\")\n",
        "else:\n",
        "    print(\"ğŸš« å·²ç¦ç”¨è¯­ä¹‰å¯¹é½ï¼Œä»…ä½¿ç”¨æ—¶é—´ç²—åŒ¹é…ã€‚\")\n",
        "\n",
        "\n",
        "# ========== æƒ…æ„Ÿåˆ†æ ==========\n",
        "print(\"ğŸ­ æ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ ...\")\n",
        "df['sentiment'] = df['content'].apply(lambda x: round(SnowNLP(str(x)).sentiments, 3))\n",
        "df['minute'] = (df['aligned_time'] // 60).astype(int)\n",
        "print(\"âœ… æƒ…æ„Ÿåˆ†æå®Œæˆ\")\n",
        "\n",
        "# ========== è¾“å‡ºç»“æœ ==========\n",
        "df.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(df.to_dict(orient=\"records\"), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\nğŸ‰ å…¨éƒ¨å®Œæˆï¼\")\n",
        "print(f\"ğŸ“„ å¯¹é½ä¸æƒ…æ„Ÿåˆ†æç»“æœ (CSV)ï¼š{out_csv}\")\n",
        "print(f\"ğŸ“˜ å¯¹é½ä¸æƒ…æ„Ÿåˆ†æç»“æœ (JSON)ï¼š{out_json}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dafc30b",
      "metadata": {},
      "source": [
        "## Step 4ï¼šæ•°æ®æ·±åº¦æ¸…æ´—â€”â€”åˆ é™¤æ— ç”¨æ ã€é›†åˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b0b366",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# åˆå¹¶ talkshow 12 é›†ï¼ˆå«ç¬¬ 12 é›†ä¸Šä¸­ä¸‹ï¼‰æ‰€æœ‰ *_timealigned_view.csv ä¸ºä¸€å¼ â€œé•¿è¡¨â€\n",
        "# ä½œè€…ï¼šä½ çš„ç ”ç©¶åŠ©æ‰‹\n",
        "# éœ€æ±‚ä¾èµ–ï¼špip install pandas numpy\n",
        "\n",
        "import os, re, glob, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ------------ 1) é…ç½® ------------\n",
        "BASE_DIR = r\"C:\\Users\\koto\\Desktop\\dataset\"     # ä½ çš„æ•°æ®æ–‡ä»¶å¤¹\n",
        "OUT_DIR  = os.path.join(BASE_DIR, \"talkshow_dataset\")\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# åªåˆå¹¶ *_timealigned_view.csvï¼ˆæ›´å¹²å‡€ï¼‰\n",
        "PATTERNS = [\"*timealigned_view.csv\", \"*timealign*_view.csv\"]\n",
        "\n",
        "# ------------ 2) é€šç”¨å·¥å…· ------------\n",
        "def try_read_csv(path):\n",
        "    \"\"\"å¤šç¼–ç å®¹é”™è¯»å–ï¼Œä¼˜å…ˆ utf-8-sig -> utf-8 -> gb18030\"\"\"\n",
        "    last_err = None\n",
        "    for enc in [\"utf-8-sig\", \"utf-8\", \"gb18030\"]:\n",
        "        try:\n",
        "            return pd.read_csv(path, encoding=enc)\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    raise last_err\n",
        "\n",
        "def norm(s):\n",
        "    return str(s).strip().lower().replace(\"_\",\" \").replace(\"-\",\" \")\n",
        "\n",
        "def find_col(df, target, contains_ok=True):\n",
        "    \"\"\"æ¨¡ç³Šæ‰¾åˆ—åï¼Œæ”¯æŒ content / mapped text / mapped_text ç­‰å˜ä½“\"\"\"\n",
        "    t = norm(target)\n",
        "    for c in df.columns:\n",
        "        if norm(c) == t:\n",
        "            return c\n",
        "    if contains_ok:\n",
        "        for c in df.columns:\n",
        "            if t in norm(c):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def to_seconds(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s = str(x).strip()\n",
        "    # å·²ç»æ˜¯æ•°å€¼\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        pass\n",
        "    # æ”¯æŒ mm:ss / hh:mm:ss\n",
        "    parts = s.split(\":\")\n",
        "    try:\n",
        "        if len(parts)==3:\n",
        "            h,m,rest = parts\n",
        "            return int(h)*3600 + int(m)*60 + float(rest)\n",
        "        elif len(parts)==2:\n",
        "            m,rest = parts\n",
        "            return int(m)*60 + float(rest)\n",
        "    except:\n",
        "        return np.nan\n",
        "    return np.nan\n",
        "\n",
        "def parse_ep_and_part(fname):\n",
        "    \"\"\"\n",
        "    ä»æ–‡ä»¶åæå–é›†æ•°ä¸åˆ†æ®µï¼š\n",
        "    - talkshow01s_... -> ep=1, part='s'\n",
        "    - talkshow12m_... -> ep=12, part='m'\n",
        "    - è‹¥ç¼ºå°‘åˆ†æ®µåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²\n",
        "    \"\"\"\n",
        "    base = os.path.basename(fname)\n",
        "    m = re.search(r\"talkshow(\\d{2})([smx])?_.*timealign.*_view\\.csv\", base, flags=re.IGNORECASE)\n",
        "    if not m:\n",
        "        # å…¼å®¹æ›´å®½æ¾çš„æƒ…å†µ\n",
        "        m = re.search(r\"talkshow(\\d{2})([smx])?\", base, flags=re.IGNORECASE)\n",
        "    ep_num = int(m.group(1)) if m else -1\n",
        "    part   = (m.group(2) or \"\").lower() if m else \"\"\n",
        "    part_cn = {\"s\":\"ä¸Š\", \"m\":\"ä¸­\", \"x\":\"ä¸‹\"}.get(part, \"\")\n",
        "    return ep_num, part, part_cn, base\n",
        "\n",
        "# ------------ 3) æ‰«ææ–‡ä»¶ ------------\n",
        "csv_paths = []\n",
        "for pat in PATTERNS:\n",
        "    csv_paths += glob.glob(os.path.join(BASE_DIR, pat))\n",
        "csv_paths = sorted(set(csv_paths))\n",
        "\n",
        "if not csv_paths:\n",
        "    raise SystemExit(f\"æ²¡æœ‰æ‰¾åˆ° *_timealign*_view.csvï¼Œç›®å½•ï¼š{BASE_DIR}\")\n",
        "\n",
        "print(f\"å‘ç° {len(csv_paths)} ä¸ªæ–‡ä»¶ï¼š\")\n",
        "for p in csv_paths:\n",
        "    print(\" -\", os.path.basename(p))\n",
        "\n",
        "# ------------ 4) é€æ–‡ä»¶è¯»å–å¹¶æ ‡å‡†åŒ– ------------\n",
        "merged_rows = []\n",
        "summary_rows = []\n",
        "\n",
        "for path in csv_paths:\n",
        "    df = try_read_csv(path)\n",
        "\n",
        "    # æ‰¾å…³é”®åˆ—ï¼ˆä¸åŒç‰ˆæœ¬å¯èƒ½å«æ³•ç•¥ä¸åŒï¼‰\n",
        "    c_content  = find_col(df, \"content\")\n",
        "    c_dm_mmss  = find_col(df, \"offset_time_mmss\")\n",
        "    c_sub_text = find_col(df, \"mapped text\") or find_col(df, \"mapped_text\")\n",
        "    c_sub_mmss = find_col(df, \"mapped_time_mmss\")\n",
        "\n",
        "    need = [c_content, c_dm_mmss, c_sub_text, c_sub_mmss]\n",
        "    if not all(need):\n",
        "        print(f\"[è·³è¿‡] åˆ—åä¸å®Œæ•´ï¼š{os.path.basename(path)}\")\n",
        "        continue\n",
        "\n",
        "    # ç»Ÿä¸€é‡å‘½å\n",
        "    df_std = pd.DataFrame({\n",
        "        \"content\":         df[c_content].astype(str),\n",
        "        \"offset_time_mmss\":df[c_dm_mmss],\n",
        "        \"mapped_text\":     df[c_sub_text].astype(str),\n",
        "        \"mapped_time_mmss\":df[c_sub_mmss],\n",
        "    })\n",
        "\n",
        "    # ç§’æ•°åˆ—ï¼ˆè‹¥åŸè¡¨å·²æœ‰ *_sec ä¹Ÿæ²¡å…³ç³»ï¼Œè¿™é‡Œç»Ÿä¸€è®¡ç®—ï¼Œç¡®ä¿ä¸€è‡´ï¼‰\n",
        "    df_std[\"dm_time_s\"]  = df_std[\"offset_time_mmss\"].apply(to_seconds)\n",
        "    df_std[\"sub_time_s\"] = df_std[\"mapped_time_mmss\"].apply(to_seconds)\n",
        "\n",
        "    # æ ‡æ³¨é›†æ•°/åˆ†æ®µ\n",
        "    ep_num, part, part_cn, base = parse_ep_and_part(path)\n",
        "    df_std[\"ep_num\"]   = ep_num\n",
        "    df_std[\"ep_label\"] = f\"EP{ep_num:02d}\" if ep_num>0 else \"EP??\"\n",
        "    df_std[\"ep_part\"]  = part              # s/m/x\n",
        "    df_std[\"ep_part_cn\"] = part_cn         # ä¸Š/ä¸­/ä¸‹ï¼ˆç¬¬12é›†ä¼šç”¨åˆ°ï¼‰\n",
        "\n",
        "    # è®°å½•\n",
        "    merged_rows.append(df_std)\n",
        "    summary_rows.append({\n",
        "        \"file\": base,\n",
        "        \"ep_num\": ep_num,\n",
        "        \"ep_label\": f\"EP{ep_num:02d}\" if ep_num>0 else \"EP??\",\n",
        "        \"ep_part\": part,\n",
        "        \"ep_part_cn\": part_cn,\n",
        "        \"rows\": len(df_std)\n",
        "    })\n",
        "\n",
        "# åˆå¹¶\n",
        "if not merged_rows:\n",
        "    raise SystemExit(\"æœªæœ‰å¯åˆå¹¶çš„æ•°æ®ï¼Œè¯·æ£€æŸ¥åˆ—åæˆ–æ–‡ä»¶æ ¼å¼ã€‚\")\n",
        "merged = pd.concat(merged_rows, ignore_index=True)\n",
        "\n",
        "# å»é™¤æ˜æ˜¾é‡å¤ï¼ˆåŒä¸€é›†/åˆ†æ®µï¼ŒåŒä¸€å¼¹å¹•å’Œå­—å¹•æ—¶é—´&æ–‡æœ¬å®Œå…¨ä¸€è‡´ï¼‰\n",
        "merged = merged.drop_duplicates(subset=[\n",
        "    \"ep_num\",\"ep_part\",\"dm_time_s\",\"sub_time_s\",\"content\",\"mapped_text\"\n",
        "]).reset_index(drop=True)\n",
        "\n",
        "# ------------ 5) å¯¼å‡º ------------\n",
        "out_csv = os.path.join(OUT_DIR, \"talkshow_dataset.csv\")\n",
        "merged.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "summary = pd.DataFrame(summary_rows).sort_values([\"ep_num\",\"ep_part\"])\n",
        "summary_out = os.path.join(OUT_DIR, \"intro_talkshow_dataset.csv\")\n",
        "summary.to_csv(summary_out, index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "# ç®€è¦ç»Ÿè®¡æ‰“å°\n",
        "print(\"\\nåˆå¹¶å®Œæˆï¼\")\n",
        "print(\"è¾“å‡ºæ–‡ä»¶ï¼š\", out_csv)\n",
        "print(\"æ±‡æ€»æ¸…å•ï¼š\", summary_out)\n",
        "print(\"\\nå„é›†è¡Œæ•°é¢„è§ˆï¼š\")\n",
        "print(summary.groupby([\"ep_label\",\"ep_part\"]).agg(rows=(\"rows\",\"sum\")).reset_index().head(20))\n",
        "\n",
        "# å¦‚æœä½ æƒ³ç¡®è®¤ç¬¬12é›†çš„ä¸‰æ®µæ˜¯å¦éƒ½åœ¨ï¼š\n",
        "if (summary['ep_num']==12).any():\n",
        "    print(\"\\nç¬¬ 12 é›†åˆ†æ®µç»Ÿè®¡ï¼š\")\n",
        "    print(summary[summary['ep_num']==12][[\"file\",\"ep_part\",\"ep_part_cn\",\"rows\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d93beef",
      "metadata": {},
      "source": [
        "## Step 5ï¼šæ•°æ®å¯è§†åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bda487c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================= 0) ï¼ˆå¯é€‰ï¼‰è‡ªåŠ¨å®‰è£…ç¼ºå¤±ä¾èµ– =======================\n",
        "import sys, subprocess, importlib\n",
        "def _ensure(pkgs):\n",
        "    for p in pkgs:\n",
        "        try: importlib.import_module(p.split(\"==\")[0])\n",
        "        except Exception:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p, \"-q\"])\n",
        "_ensure([\"pandas\",\"numpy\",\"plotly\",\"ipywidgets\",\"pyarrow\"])  # opencc å¯é€‰\n",
        "\n",
        "# ======================= 1) å¯¼å…¥ä¸å…¨å±€é…ç½® =======================\n",
        "import os, re, math, unicodedata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "\n",
        "pd.options.mode.copy_on_write = True\n",
        "pio.renderers.default = \"notebook_connected\"  # VSCode/Jupyter å‡å¯\n",
        "pio.templates.default = \"simple_white\"\n",
        "\n",
        "# --------- è·¯å¾„ä¸å­—ä½“ ---------\n",
        "DATA_PATH           = r\"C:\\Users\\koto\\Desktop\\talkshow_dataset.csv\"\n",
        "CACHE_PARQUET       = r\"C:\\Users\\koto\\Desktop\\talkshow_cache_fast.parquet\"\n",
        "FONT_FAMILY         = \"Microsoft YaHei, SimHei, Arial\"\n",
        "\n",
        "# ä½ æä¾›çš„æˆå“ HTMLï¼ˆå¦‚åœ¨ Linux/äº‘ç«¯ï¼Œè¯·æ”¹ä¸ºå®é™…è·¯å¾„ï¼‰\n",
        "HTML_HEATMAP_PATH   = r\"/mnt/data/danmu_heatmap.html\"\n",
        "HTML_WORDCLOUD_PATH = r\"/mnt/data/interactive_sector_wordcloud(1).html\"\n",
        "\n",
        "# --------- æé€Ÿé¢„è§ˆå‚æ•°ï¼ˆåæœŸå‡ºæœ€ç»ˆå›¾å†è°ƒé«˜ç²¾åº¦ï¼‰ ---------\n",
        "FAST_PREVIEW       = True     # æœ€ç»ˆç‰ˆæ”¹ä¸º False\n",
        "USE_OPENCC         = False    # éœ€è¦ç¹â†’ç®€æ—¶è®¾ Trueï¼ˆéœ€å®‰è£… openccï¼‰\n",
        "BIN_SEC_DEFAULT    = 8        # æ—¶é—´çº¿åˆ†ç®±ï¼ˆç§’ï¼‰\n",
        "WIN_SEC_DEFAULT    = 4        # å³°å€¼çª—å£ Â± ç§’\n",
        "HEATMAP_PCT_BINS   = 36       # 0-100% è¿›åº¦çƒ­åŠ›å›¾æ¨ªå‘æ ¼\n",
        "BASE_SAMPLE        = 4000     # å…¨å±€å¯¹ç…§æŠ½æ ·ä¸Šé™ï¼ˆè¶Šå¤§è¶Šæ…¢ï¼‰\n",
        "TOP_N_TERMS        = 12       # å…³é”®è¯å±•ç¤ºæ¡æ•°\n",
        "\n",
        "# --------- ç»Ÿä¸€ä¸»é¢˜è‰² ---------\n",
        "THEME = dict(\n",
        "    line=\"#2F5597\",    # æ—¶é—´çº¿\n",
        "    marker=\"#F18F01\",  # å³°å€¼ç‚¹\n",
        "    vline=\"#E74C3C\",   # æ’­æ”¾çº¿\n",
        "    heat_seq=px.colors.sequential.YlGnBu,  # çƒ­åŠ›å›¾\n",
        "    pos=\"#2ECC71\",\n",
        "    neg=\"#E74C3C\",\n",
        "    conf=\"#7F8C8D\",\n",
        "    app=\"#9B59B6\",\n",
        "    text=\"#222222\",\n",
        ")\n",
        "\n",
        "# ======================= 2) è¯»æ•° & é¢„å¤„ç†ï¼ˆå¸¦ç¼“å­˜ï¼‰ =======================\n",
        "def read_csv_ch(path):\n",
        "    for enc in [\"utf-8-sig\",\"utf-8\",\"gb18030\"]:\n",
        "        try: return pd.read_csv(path, encoding=enc)\n",
        "        except Exception: pass\n",
        "    return pd.read_csv(path, encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "def to_seconds(x):\n",
        "    if pd.isna(x): return np.nan\n",
        "    s=str(x).strip()\n",
        "    try: return float(s)\n",
        "    except Exception: pass\n",
        "    p=s.split(\":\")\n",
        "    try:\n",
        "        if len(p)==3: return int(p[0])*3600+int(p[1])*60+float(p[2])\n",
        "        if len(p)==2: return int(p[0])*60+float(p[1])\n",
        "    except Exception: return np.nan\n",
        "    return np.nan\n",
        "\n",
        "def to_simplified(s):\n",
        "    s = unicodedata.normalize(\"NFKC\", str(s))\n",
        "    if USE_OPENCC:\n",
        "        try:\n",
        "            from opencc import OpenCC\n",
        "            s = OpenCC(\"t2s\").convert(s)\n",
        "        except Exception: pass\n",
        "    s = re.sub(r\"[\\u200b-\\u200f\\u202a-\\u202e]\", \"\", s)\n",
        "    return s\n",
        "\n",
        "def pick_col(df, name):\n",
        "    tgt = name.lower().replace(\"_\",\" \")\n",
        "    for c in df.columns:\n",
        "        cc = c.lower().replace(\"_\",\" \")\n",
        "        if cc==tgt or tgt in cc: return c\n",
        "    return None\n",
        "\n",
        "def load_data(csv_path, cache_path):\n",
        "    if os.path.exists(cache_path):\n",
        "        return pd.read_parquet(cache_path)\n",
        "\n",
        "    raw = read_csv_ch(csv_path)\n",
        "    c_content  = pick_col(raw, \"content\")\n",
        "    c_dm_mmss  = pick_col(raw, \"offset_time_mmss\")\n",
        "    c_sub_text = pick_col(raw, \"mapped text\") or pick_col(raw,\"mapped_text\")\n",
        "    c_sub_mmss = pick_col(raw, \"mapped_time_mmss\")\n",
        "    c_ep_num   = pick_col(raw, \"ep_num\")\n",
        "\n",
        "    assert all([c_content,c_dm_mmss,c_sub_text,c_sub_mmss]), \\\n",
        "        \"ç¼ºå°‘å¿…é¡»åˆ—ï¼šcontent / offset_time_mmss / mapped_text / mapped_time_mmss\"\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"content\":     raw[c_content].astype(str),\n",
        "        \"dm_time_s\":   raw[c_dm_mmss].apply(to_seconds),\n",
        "        \"mapped_text\": raw[c_sub_text].astype(str),\n",
        "        \"sub_time_s\":  raw[c_sub_mmss].apply(to_seconds),\n",
        "        \"ep_num\":      (raw[c_ep_num] if c_ep_num else 1),\n",
        "    })\n",
        "    df[\"ep_num\"]   = df[\"ep_num\"].fillna(1).astype(int)\n",
        "    df[\"ep_label\"] = df[\"ep_num\"].map(lambda x: f\"EP{x:02d}\")\n",
        "    df = df.dropna(subset=[\"dm_time_s\",\"sub_time_s\"]).reset_index(drop=True)\n",
        "\n",
        "    if USE_OPENCC:\n",
        "        df[\"content\"]     = df[\"content\"].map(to_simplified)\n",
        "        df[\"mapped_text\"] = df[\"mapped_text\"].map(to_simplified)\n",
        "\n",
        "    # æƒ…ç»ª/ç¬‘ç±»æ­£åˆ™ï¼ˆå‘é‡åŒ–ï¼‰\n",
        "    def _re(pats): return re.compile(\"|\".join(pats), re.IGNORECASE)\n",
        "    LAUGH_RE    = _re([r\"(å“ˆ){2,}\", r\"(å‘µ){2,}\", r\"ç¬‘(æ­»|ç–¯|ç¿»|è£‚|å–·|å“­)?\", r\"233+\", r\"hhh+\", r\"xswl\", r\"ä¹(æ­»|ç–¯|å)\"])\n",
        "    POSITIVE_RE = _re([r\"å¥½ç¬‘\", r\"ç»äº†\", r\"å¤ª?é€—äº†\", r\"æœ‰æ¢—\", r\"é«˜èƒ½\", r\"å‰å®³\", r\"å¤©æ‰\"])\n",
        "    NEGATIVE_RE = _re([r\"ä¸å¥½ç¬‘\", r\"æ— èŠ\", r\"çƒ‚\", r\"å·®åŠ²\", r\"å†·åœº\", r\"å°´å°¬\", r\"æ‹‰èƒ¯\", r\"ä½ä¿—\"])\n",
        "    CONFUSE_RE  = _re([r\"\\?{2,}\", r\"å•¥æ„æ€\", r\"æ²¡æ‡‚\", r\"ä¸æ‡‚\"])\n",
        "    APPLAUSE_RE = _re([r\"é¼“æŒ\", r\"æŒå£°\", r\"æ‹æ‰‹\", r\"respect\"])\n",
        "\n",
        "    s = df[\"content\"]\n",
        "    df[\"is_laugh\"]    = s.str.contains(LAUGH_RE,    regex=True).astype(int)\n",
        "    df[\"is_positive\"] = s.str.contains(POSITIVE_RE, regex=True).astype(int)\n",
        "    df[\"is_negative\"] = s.str.contains(NEGATIVE_RE, regex=True).astype(int)\n",
        "    df[\"is_confuse\"]  = s.str.contains(CONFUSE_RE,  regex=True).astype(int)\n",
        "    df[\"is_applause\"] = s.str.contains(APPLAUSE_RE, regex=True).astype(int)\n",
        "    df[\"lag_s\"]       = df[\"dm_time_s\"] - df[\"sub_time_s\"]\n",
        "\n",
        "    # ç˜¦èº«åç¼“å­˜\n",
        "    keep = [\"content\",\"dm_time_s\",\"mapped_text\",\"sub_time_s\",\"ep_num\",\"ep_label\",\n",
        "            \"is_laugh\",\"is_positive\",\"is_negative\",\"is_confuse\",\"is_applause\",\"lag_s\"]\n",
        "    df = df[keep]\n",
        "    try: df.to_parquet(cache_path, index=False)\n",
        "    except Exception: pass\n",
        "    return df\n",
        "\n",
        "assert os.path.exists(DATA_PATH), f\"æ‰¾ä¸åˆ°æ•°æ®ï¼š{DATA_PATH}\"\n",
        "df = load_data(DATA_PATH, CACHE_PARQUET)\n",
        "\n",
        "EP_OPTS   = sorted(df[\"ep_num\"].unique().tolist())\n",
        "EP_LABELS = dict(df[[\"ep_num\",\"ep_label\"]].drop_duplicates().values.tolist())\n",
        "\n",
        "# ======================= 3) ç»Ÿè®¡ & å¯è§†åŒ–å·¥å…· =======================\n",
        "def laughing_signal(frame):\n",
        "    # å…¨å±€ç»Ÿä¸€çš„â€œåŠ æƒç¬‘æŒ‡æ•°â€\n",
        "    return (frame[\"is_laugh\"] \n",
        "            + 0.5*frame[\"is_positive\"] \n",
        "            + 0.7*frame[\"is_applause\"] \n",
        "            - 0.4*frame[\"is_negative\"] \n",
        "            - 0.3*frame[\"is_confuse\"])\n",
        "\n",
        "def bin_series(g, bin_sec, metric=\"weighted\"):\n",
        "    if g.empty: return pd.DataFrame(columns=[\"t\",\"count\"])\n",
        "    tbin = (g[\"dm_time_s\"]//bin_sec).astype(int)\n",
        "    if metric==\"weighted\":\n",
        "        vals = laughing_signal(g).groupby(tbin).sum()\n",
        "    else:\n",
        "        vals = g.groupby(tbin)[\"is_laugh\"].sum()\n",
        "    return pd.DataFrame({\"t\": vals.index.values*bin_sec, \"count\": vals.values})\n",
        "\n",
        "def gaussian_smooth(y, k=7):\n",
        "    # k ä¸ºå¥‡æ•°æ›´å¥½ï¼›å°äº3ç›´æ¥è¿”å›\n",
        "    k = int(max(3, k))\n",
        "    if k % 2 == 0: k += 1\n",
        "    if len(y) < k: return y\n",
        "    xs = np.arange(k) - (k-1)/2\n",
        "    sigma = max(1.0, k/2.5)\n",
        "    kernel = np.exp(-0.5*(xs/sigma)**2)\n",
        "    kernel /= kernel.sum()\n",
        "    return np.convolve(y, kernel, mode=\"same\")\n",
        "\n",
        "def find_peaks(x, y, top_n=3, min_dist_bins=3, min_prom=0.0):\n",
        "    # å…ˆæ‰¾å±€éƒ¨æå¤§ï¼Œå†æŒ‰ prominence æ’åºï¼Œæœ€ååšæœ€å°é—´è·ç­›é€‰\n",
        "    n = len(y)\n",
        "    if n < 3: return []\n",
        "    # å€™é€‰\n",
        "    cand = []\n",
        "    for i in range(1, n-1):\n",
        "        if y[i] >= y[i-1] and y[i] >= y[i+1] and y[i] > 0:\n",
        "            # å·¦å³æœ€å°å€¼\n",
        "            li = i-1\n",
        "            mleft = y[i]\n",
        "            while li >= 0 and y[li] <= y[li+1]:\n",
        "                mleft = min(mleft, y[li]); li -= 1\n",
        "            ri = i+1\n",
        "            mright = y[i]\n",
        "            while ri < n and y[ri] <= y[ri-1]:\n",
        "                mright = min(mright, y[ri]); ri += 1\n",
        "            prom = y[i] - max(mleft, mright)\n",
        "            if prom >= min_prom:\n",
        "                cand.append((i, y[i], prom))\n",
        "    # prominenceä¼˜å…ˆï¼Œå…¶æ¬¡é«˜åº¦\n",
        "    cand.sort(key=lambda t: (t[2], t[1]), reverse=True)\n",
        "    chosen = []\n",
        "    for i, h, prom in cand:\n",
        "        xi = x[i]\n",
        "        # è·ç¦»ç­›é€‰ï¼ˆä»¥ bin ä¸ªæ•°ä¸ºå•ä½ï¼‰\n",
        "        ok = True\n",
        "        for cx, ch in chosen:\n",
        "            if abs(i - np.searchsorted(x, cx)) < min_dist_bins:\n",
        "                ok = False; break\n",
        "        if ok:\n",
        "            chosen.append((xi, h))\n",
        "        if len(chosen) >= max(1, top_n):\n",
        "            break\n",
        "    return chosen\n",
        "\n",
        "STOP = set(list(\"å“ˆå“ˆå‘µå‘µå•Šå‘€å“¦æ©å—¯è¿™è¿™ä¸ªé‚£ä¸ªä¸€ä¸ªä»€ä¹ˆæ€ä¹ˆå°±æ˜¯å¯ä»¥çœŸçš„ç„¶åæ‰€ä»¥ä¸ä¼šä¸æ˜¯æˆ‘ä»¬ä½ ä»¬ä»–ä»¬è€Œä¸”ä½†æ˜¯è¿˜æœ‰å•¦å§å˜›å‘¢\"))\n",
        "def tokenize_bi(s):\n",
        "    s = re.sub(r\"[^\\u4e00-\\u9fffA-Za-z0-9]+\",\"\", str(s))\n",
        "    toks = re.findall(r'(?=([\\u4e00-\\u9fff]{2}))', s)  # lookahead äºŒå­—è¯\n",
        "    return [w for w in toks if (w not in STOP) and (w[0]!=w[1])]\n",
        "\n",
        "def keywords_peak_vs_base(df_ep, peak_center, win_sec, topN=TOP_N_TERMS, base_sample=BASE_SAMPLE):\n",
        "    win = df_ep[(df_ep[\"sub_time_s\"]>=peak_center-win_sec) & (df_ep[\"sub_time_s\"]<=peak_center+win_sec)]\n",
        "    mask = (df_ep[\"is_laugh\"]|df_ep[\"is_positive\"]|df_ep[\"is_applause\"]).astype(bool)\n",
        "    base = df_ep[mask & ~df_ep.index.isin(win.index)]\n",
        "    if len(base)>base_sample:\n",
        "        base = base.sample(base_sample, random_state=42)\n",
        "\n",
        "    cp, cb = Counter(), Counter()\n",
        "    for s in win[\"content\"]:  cp.update(tokenize_bi(s))\n",
        "    for s in base[\"content\"]: cb.update(tokenize_bi(s))\n",
        "\n",
        "    eps=1e-9; Np=sum(cp.values())+eps; Nb=sum(cb.values())+eps\n",
        "    rows=[(t, math.log((c/Np+eps)/(cb.get(t,0)/Nb+eps))) for t,c in cp.items()]\n",
        "    rows.sort(key=lambda x:x[1], reverse=True)\n",
        "    return pd.DataFrame(rows[:topN], columns=[\"term\",\"score\"])\n",
        "\n",
        "def aggregate_subs(df_ep, center, win_sec):\n",
        "    sub = df_ep[(df_ep[\"sub_time_s\"]>=center-win_sec) & (df_ep[\"sub_time_s\"]<=center+win_sec)].copy()\n",
        "    g = sub.groupby([\"mapped_text\",\"sub_time_s\"], as_index=False).agg(\n",
        "        win_laugh=(\"is_laugh\",\"sum\"),\n",
        "        win_pos=(\"is_positive\",\"sum\"),\n",
        "        win_neg=(\"is_negative\",\"sum\"),\n",
        "        win_conf=(\"is_confuse\",\"sum\"),\n",
        "        win_app=(\"is_applause\",\"sum\"),\n",
        "        median_lag=(\"lag_s\",\"median\")\n",
        "    )\n",
        "    g[\"laugh_score\"] = g[\"win_laugh\"] + 0.5*g[\"win_pos\"] + 0.7*g[\"win_app\"] \\\n",
        "                       - 0.8*g[\"win_neg\"] - 0.5*g[\"win_conf\"]\n",
        "    return g.sort_values(\"laugh_score\", ascending=False)\n",
        "\n",
        "def heatmap_percent(df_all, pct_bins=HEATMAP_PCT_BINS, metric=\"weighted\", basis=\"dm\", norm=\"raw\"):\n",
        "    # basis: \"dm\" ä½¿ç”¨ dm_time_sï¼›\"sub\" ä½¿ç”¨ sub_time_s\n",
        "    tcol = \"dm_time_s\" if basis==\"dm\" else \"sub_time_s\"\n",
        "    out=[]\n",
        "    labels=[]\n",
        "    for ep, g in df_all.groupby(\"ep_num\"):\n",
        "        g = g.sort_values(tcol)\n",
        "        t0, t1 = g[tcol].min(), g[tcol].max()\n",
        "        dur = max(t1 - t0, 1e-6)\n",
        "        gg = g.copy()\n",
        "        gg[\"pct_bin\"] = ((gg[tcol]-t0)/dur * pct_bins).clip(0,pct_bins-1).astype(int)\n",
        "        if metric==\"weighted\":\n",
        "            vals = laughing_signal(gg)\n",
        "        else:\n",
        "            vals = gg[\"is_laugh\"]\n",
        "        cnt = vals.groupby(gg[\"pct_bin\"]).sum()\n",
        "        row = np.array([float(cnt.get(i,0.0)) for i in range(pct_bins)], dtype=float)\n",
        "        if norm==\"row\":\n",
        "            m = row.max()\n",
        "            row = row/m if m>0 else row\n",
        "        out.append(row); labels.append(f\"EP{ep:02d}\")\n",
        "    if not out:\n",
        "        return np.zeros((0, pct_bins)), []\n",
        "    mat = np.vstack(out)\n",
        "    return mat, labels\n",
        "\n",
        "# ======================= 4) äº¤äº’æ§ä»¶ï¼ˆipywidgetsï¼‰ =======================\n",
        "dd_ep     = widgets.Dropdown(options=[(EP_LABELS.get(e,f\"EP{e:02d}\"),e) for e in EP_OPTS], value=EP_OPTS[0], description=\"é›†æ•°\")\n",
        "sl_bin    = widgets.IntSlider(min=2,max=20,step=1,value=BIN_SEC_DEFAULT,description=\"åˆ†ç®±(s)\")\n",
        "sl_win    = widgets.IntSlider(min=2,max=12, step=1,value=WIN_SEC_DEFAULT,description=\"çª—å£Â±s\")\n",
        "sl_peak   = widgets.IntSlider(min=1,max=8, step=1,value=3, description=\"å³°å€¼æ•°\")\n",
        "sl_smooth = widgets.IntSlider(min=3,max=21,step=2,value=7, description=\"å¹³æ»‘çª—\")\n",
        "sl_prom   = widgets.FloatSlider(min=0.0,max=5.0,step=0.1,value=0.5, description=\"æ˜¾è‘—æ€§â‰¥\")\n",
        "sl_mdist  = widgets.IntSlider(min=1,max=10,step=1,value=3, description=\"æœ€å°é—´è·\")\n",
        "tg_metric = widgets.ToggleButtons(options=[(\"çº¯ç¬‘\",\"raw\"),(\"åŠ æƒ\",\"weighted\")], value=\"weighted\", description=\"æŒ‡æ ‡\")\n",
        "tg_basis  = widgets.ToggleButtons(options=[(\"å¼¹å¹•æ—¶é—´\",\"dm\"),(\"å­—å¹•æ—¶é—´\",\"sub\")], value=\"dm\", description=\"æ—¶é—´åŸºå‡†\")\n",
        "dd_hnorm  = widgets.Dropdown(options=[(\"åŸå§‹\",\"raw\"),(\"è¡Œå½’ä¸€åŒ–(0-1)\",\"row\")], value=\"raw\", description=\"çƒ­åŠ›å›¾\")\n",
        "\n",
        "play      = widgets.Play(value=0, min=0, max=1000, step=10, interval=400, description=\"Play\")\n",
        "sl_play   = widgets.FloatSlider(min=0,max=1,step=0.001,value=0, readout=False)\n",
        "widgets.jslink((play,\"value\"), (sl_play,\"value\"))\n",
        "\n",
        "# è¾“å‡ºåŒº\n",
        "out_timeline = widgets.Output()\n",
        "out_kw       = widgets.Output()\n",
        "out_subs     = widgets.Output()\n",
        "out_table    = widgets.Output()\n",
        "out_heat     = widgets.Output()\n",
        "out_caption  = widgets.Output()\n",
        "out_html1    = widgets.Output()\n",
        "out_html2    = widgets.Output()\n",
        "\n",
        "def render_html_file(path, out):\n",
        "    with out:\n",
        "        clear_output(wait=True)\n",
        "        try:\n",
        "            if path and os.path.exists(path):\n",
        "                html = open(path, \"r\", encoding=\"utf-8\").read()\n",
        "                display(HTML(html))\n",
        "            else:\n",
        "                display(HTML(f\"<div style='font-family:{FONT_FAMILY}'>æ‰¾ä¸åˆ° HTMLï¼š<code>{path}</code></div>\"))\n",
        "        except Exception as e:\n",
        "            display(HTML(f\"<div style='font-family:{FONT_FAMILY};color:#b00'>HTML åŠ è½½å¤±è´¥ï¼š{e}</div>\"))\n",
        "\n",
        "# ======================= 5) æ¸²æŸ“å‡½æ•° =======================\n",
        "def render_all(*_):\n",
        "    ep       = dd_ep.value\n",
        "    bin_sec  = sl_bin.value\n",
        "    win_sec  = sl_win.value\n",
        "    n_peaks  = sl_peak.value\n",
        "    k_smooth = sl_smooth.value\n",
        "    min_prom = sl_prom.value\n",
        "    min_dist = sl_mdist.value\n",
        "    metric   = tg_metric.value\n",
        "    basis    = tg_basis.value\n",
        "    hnorm    = dd_hnorm.value\n",
        "    playhead = sl_play.value\n",
        "\n",
        "    g = df[df[\"ep_num\"]==ep]\n",
        "    s = bin_series(g, bin_sec, metric=metric)\n",
        "    y = gaussian_smooth(s[\"count\"].values, k=k_smooth); x=s[\"t\"].values\n",
        "\n",
        "    # å³°å€¼ï¼ˆæŒ‰ prominence & æœ€å°é—´è·ï¼‰ï¼Œå¹¶é€‰æ‹©ç¦»æ’­æ”¾ä½ç½®æœ€è¿‘çš„ä¸€ä¸ªç”¨äºè§£é‡Š\n",
        "    peaks = find_peaks(x, y, top_n=n_peaks, min_dist_bins=min_dist, min_prom=min_prom)\n",
        "    if len(g)>0:\n",
        "        tmin, tmax = g[\"dm_time_s\"].min(), g[\"dm_time_s\"].max()\n",
        "        cur_t = tmin + playhead*(tmax - tmin)\n",
        "    else:\n",
        "        cur_t = 0.0\n",
        "    if peaks:\n",
        "        peak_t = min(peaks, key=lambda p: abs(p[0]-cur_t))[0]\n",
        "    else:\n",
        "        peak_t = x[int(len(x)*0.5)] if len(x)>0 else 0.0\n",
        "\n",
        "    # 1) æ—¶é—´çº¿\n",
        "    with out_timeline:\n",
        "        clear_output(wait=True)\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=x, y=y, mode=\"lines\", name=\"ç¬‘å¯†åº¦ï¼ˆå¹³æ»‘ï¼‰\",\n",
        "                                 line=dict(color=THEME[\"line\"], width=2)))\n",
        "        for i,(tx,ty) in enumerate(peaks,1):\n",
        "            fig.add_trace(go.Scatter(x=[tx], y=[ty], mode=\"markers+text\",\n",
        "                                     text=[f\"#{i}\"], textposition=\"top center\",\n",
        "                                     marker=dict(size=9, color=THEME[\"marker\"]),\n",
        "                                     name=f\"å³°å€¼{i}\"))\n",
        "        fig.add_vline(x=cur_t, line=dict(color=THEME[\"vline\"], width=2, dash=\"dot\"))\n",
        "        fig.update_layout(font=dict(family=FONT_FAMILY, color=THEME[\"text\"]),\n",
        "                          height=360, margin=dict(l=60,r=30,t=40,b=10),\n",
        "                          legend=dict(orientation=\"h\",y=1.02,x=0))\n",
        "        fig.update_xaxes(title_text=\"æ—¶é—´ï¼ˆç§’ï¼‰\")\n",
        "        ylabel = \"ç¬‘ç±»å¼ºåº¦ï¼ˆå¹³æ»‘, åŠ æƒï¼‰\" if metric==\"weighted\" else \"ç¬‘ç±»å¼¹å¹•æ¡æ•°ï¼ˆå¹³æ»‘ï¼‰\"\n",
        "        fig.update_yaxes(title_text=ylabel, rangemode=\"tozero\")\n",
        "        display(fig)\n",
        "\n",
        "    # 2) å³°å€¼å…³é”®è¯ï¼ˆä¸ºä»€ä¹ˆç¬‘ï¼‰ â€” æ¡å½¢æ›´å¯è¯»\n",
        "    kw = keywords_peak_vs_base(g, peak_t, win_sec, topN=TOP_N_TERMS, base_sample=BASE_SAMPLE)\n",
        "    with out_kw:\n",
        "        clear_output(wait=True)\n",
        "        kshow = kw.sort_values(\"score\")\n",
        "        figkw = px.bar(kshow, x=\"score\", y=\"term\", orientation=\"h\")\n",
        "        figkw.update_traces(marker_color=THEME[\"marker\"])\n",
        "        figkw.update_layout(height=320, margin=dict(l=100,r=20,t=10,b=10),\n",
        "                            font=dict(family=FONT_FAMILY))\n",
        "        figkw.update_xaxes(title_text=\"æ˜¾è‘—æ€§ï¼ˆå³°å€¼ç›¸å¯¹å…¨å±€å¯¹æ•°æå‡ï¼‰\")\n",
        "        figkw.update_yaxes(title_text=\"\")\n",
        "        display(figkw)\n",
        "\n",
        "    # 3) å­—å¹•å“åº”ï¼ˆå¯¹ä»€ä¹ˆå†…å®¹ç¬‘ï¼‰â€” æŒ‰ laugh_score ç€è‰²\n",
        "    subs = aggregate_subs(g, peak_t, win_sec)\n",
        "    with out_subs:\n",
        "        clear_output(wait=True)\n",
        "        show = subs.head(600) if FAST_PREVIEW else subs\n",
        "        figsc = px.scatter(\n",
        "            show, x=\"sub_time_s\", y=\"laugh_score\",\n",
        "            color=\"laugh_score\", color_continuous_scale=px.colors.sequential.Plasma,\n",
        "            hover_data=[\"mapped_text\",\"median_lag\"], height=320\n",
        "        )\n",
        "        figsc.update_xaxes(title_text=\"å­—å¹•æ—¶é—´ï¼ˆç§’ï¼‰\")\n",
        "        figsc.update_yaxes(title_text=\"laugh_scoreï¼ˆçª—å£åŠ æƒï¼‰\")\n",
        "        figsc.update_layout(font=dict(family=FONT_FAMILY), margin=dict(l=60,r=30,t=10,b=10))\n",
        "        display(figsc)\n",
        "\n",
        "    # 4) è¡¨æ ¼ Top å­—å¹•\n",
        "    with out_table:\n",
        "        clear_output(wait=True)\n",
        "        display(subs.head(20)[[\"mapped_text\",\"sub_time_s\",\"laugh_score\",\"median_lag\"]].round(3))\n",
        "\n",
        "    # 5) å…¨å­£è¿›åº¦çƒ­åŠ›å›¾ï¼ˆç»“æ„èŠ‚å¥ï¼‰\n",
        "    with out_heat:\n",
        "        clear_output(wait=True)\n",
        "        mat, labels = heatmap_percent(df, pct_bins=HEATMAP_PCT_BINS, metric=metric, basis=basis, norm=hnorm)\n",
        "        fig = go.Figure(data=go.Heatmap(z=mat, colorscale=THEME[\"heat_seq\"],\n",
        "                                        colorbar=dict(title=\"å¼ºåº¦\" if hnorm==\"row\" else \"ç¬‘ç±»æ€»é‡\")))\n",
        "        fig.update_layout(height=340, font=dict(family=FONT_FAMILY),\n",
        "                          margin=dict(l=80,r=30,t=10,b=40))\n",
        "        fig.update_yaxes(tickmode=\"array\", tickvals=list(range(len(labels))), ticktext=labels, title=\"é›†æ•°\")\n",
        "        xt = np.linspace(0, HEATMAP_PCT_BINS-1, 6, dtype=int)\n",
        "        fig.update_xaxes(tickmode=\"array\", tickvals=xt,\n",
        "                         ticktext=[f\"{int(v*100/(HEATMAP_PCT_BINS-1))}%\" for v in xt],\n",
        "                         title=\"èŠ‚ç›®è¿›åº¦ï¼ˆ%ï¼‰\")\n",
        "        display(fig)\n",
        "\n",
        "    # 6) æ–‡æœ¬è¯´æ˜\n",
        "    with out_caption:\n",
        "        clear_output(wait=True)\n",
        "        kws = \"ã€\".join(kw.sort_values(\"score\", ascending=False)[\"term\"].head(6).tolist())\n",
        "        metric_str = \"åŠ æƒç¬‘æŒ‡æ•°\" if metric==\"weighted\" else \"çº¯ç¬‘è®¡æ•°\"\n",
        "        basis_str  = \"å¼¹å¹•æ—¶é—´\" if basis==\"dm\" else \"å­—å¹•æ—¶é—´\"\n",
        "        norm_str   = \"ï¼ˆè¡Œå½’ä¸€åŒ–ï¼‰\" if hnorm==\"row\" else \"\"\n",
        "        display(HTML(f\"\"\"\n",
        "        <div style=\"font-family:{FONT_FAMILY};font-size:13px;line-height:1.6;color:{THEME['text']}\">\n",
        "        <b>å½“å‰è§£é‡Šå³°å€¼ @ {peak_t:.1f}s</b>ï¼šçª—å£å…³é”®è¯ = {kws}<br>\n",
        "        è¯»å›¾æ³•ï¼š<i>æ—¶é—´çº¿</i>å›ç­”â€œ<b>ä½•æ—¶ç¬‘</b>â€ï¼ˆ{metric_str}ï¼Œé«˜æ–¯å¹³æ»‘çª—={k_smooth}ï¼‰ï¼›<i>å…³é”®è¯</i>å›ç­”â€œ<b>ä¸ºä»€ä¹ˆç¬‘</b>â€ï¼›<i>å­—å¹•æ•£ç‚¹</i>å›ç­”â€œ<b>å¯¹ä»€ä¹ˆå†…å®¹ç¬‘</b>â€ã€‚<br>\n",
        "        å³ä¾§çƒ­åŠ›å›¾ä»¥<b>{basis_str}</b>å¯¹é½åš 0â€“100% è¿›åº¦æ¯”è¾ƒ{norm_str}ã€‚\n",
        "        </div>\n",
        "        \"\"\"))\n",
        "\n",
        "    # 7) å†…åµŒä½ æä¾›çš„ä¸¤ä¸ªæˆå“ HTMLï¼ˆå¯é€‰ï¼‰\n",
        "    render_html_file(HTML_HEATMAP_PATH, out_html1)\n",
        "    render_html_file(HTML_WORDCLOUD_PATH, out_html2)\n",
        "\n",
        "# ç»‘å®šäº‹ä»¶\n",
        "for w in (dd_ep, sl_bin, sl_win, sl_peak, sl_smooth, sl_prom, sl_mdist, tg_metric, tg_basis, dd_hnorm, sl_play):\n",
        "    w.observe(render_all, names=\"value\")\n",
        "\n",
        "# ======================= 6) é¡µé¢å¸ƒå±€ä¸é¦–æ¸²æŸ“ =======================\n",
        "controls_row1 = widgets.HBox([dd_ep, sl_bin, sl_win, sl_peak],\n",
        "                             layout=widgets.Layout(margin=\"4px 0 2px 0\"))\n",
        "controls_row2 = widgets.HBox([sl_smooth, sl_prom, sl_mdist, tg_metric, tg_basis, dd_hnorm],\n",
        "                             layout=widgets.Layout(margin=\"0 0 6px 0\"))\n",
        "controls_row3 = widgets.HBox([play, sl_play],\n",
        "                             layout=widgets.Layout(margin=\"0 0 6px 0\"))\n",
        "\n",
        "display(HTML(f\"<h3 style='font-family:{FONT_FAMILY};margin:6px 0 4px 0'>è§‚ä¼—ä¸ºä½•è€Œç¬‘ Â· äº¤äº’åˆ†æï¼ˆNotebook ç‰ˆ / æå‡ç‰ˆï¼‰</h3>\"))\n",
        "display(controls_row1, controls_row2, controls_row3)\n",
        "display(out_timeline)\n",
        "display(widgets.HBox([widgets.VBox([out_kw]), widgets.VBox([out_subs])]))\n",
        "display(out_table)\n",
        "display(out_heat)\n",
        "display(out_caption)\n",
        "display(HTML(f\"<h4 style='font-family:{FONT_FAMILY};margin:10px 0 6px 0'>è¡¥å…… Â· æˆå“ HTML é¢„è§ˆ</h4>\"))\n",
        "display(HTML(f\"<div style='font-family:{FONT_FAMILY};color:#555;margin-bottom:6px'>å¦‚éœ€æ›´æ¢è·¯å¾„ï¼Œè¯·ä¿®æ”¹é¡¶éƒ¨ <code>HTML_HEATMAP_PATH</code> / <code>HTML_WORDCLOUD_PATH</code>ã€‚</div>\"))\n",
        "display(out_html1)\n",
        "display(out_html2)\n",
        "\n",
        "render_all()  # é¦–æ¬¡æ¸²æŸ“"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
